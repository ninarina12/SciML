{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f403717",
   "metadata": {},
   "source": [
    "# Scientific machine learning\n",
    "\n",
    "## Introduction\n",
    "Scientific machine learning (SciML) combines methodologies from **machine learning** and **scientific computing** to develop models that are both **data-driven** and **domain-aware**.\n",
    "\n",
    "SciML aims to leverage the **flexibility** of machine learning methods while retaining the **interpretability** of scientific models by incorporating physical constraints or other domain knowledge.\n",
    "\n",
    "This tutorial compares two fundamental classes of SciML problems: **Neural Ordinary/Partial Differential Equations (ODEs/PDEs)** and **Physics-Informed Neural Networks (PINNs)**. The schematic below illustrates the basic conceptual differences between these two classes of problems.\n",
    "\n",
    "![schematic](schematic.png)\n",
    "\n",
    "We will make use of the [`PyTorch`](https://pytorch.org/) deep learning library to train machine learning models and perform **automatic differentiation** (essential for PINNs), and the [`torchdiffeq`](https://github.com/rtqichen/torchdiffeq) library of ODE solvers (implemented in `PyTorch`) to perform **numerical integration** (essential for neural ODEs).\n",
    "\n",
    "### References\n",
    "**Neural ODEs/PDEs**\n",
    "- [Chen, Ricky TQ, et al. \"Neural ordinary differential equations.\" *Advances in neural information processing systems* 31 (2018).](https://arxiv.org/pdf/1806.07366.pdf)\n",
    "- [Rackauckas, Christopher, et al. \"Universal differential equations for scientific machine learning.\" *arXiv preprint arXiv:2001.04385* (2020).](https://arxiv.org/pdf/2001.04385.pdf)\n",
    "\n",
    "**PINNs**\n",
    "- [Raissi, Maziar, Paris Perdikaris, and George E. Karniadakis. \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.\" *Journal of Computational physics* 378 (2019): 686-707.](https://www.sciencedirect.com/science/article/pii/S0021999118307125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd51ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "from utils import (get_batch, format_axis, props, cmap, animate_node_history, animate_pinn_history,\n",
    "                   node_inference, pinn_inference)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "default_type = torch.float64\n",
    "torch.set_default_dtype(default_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341c846",
   "metadata": {},
   "source": [
    "## Part I: Neural Differential Equations\n",
    "\n",
    "Neural differential equations are neural network models which approximate an unknown ODE or PDE from a set of observations of the solution at discrete time points. For example, consider the ODE,\n",
    "\n",
    "$$\\frac{dy}{dt} \\approx f_{\\theta}(t,y),$$\n",
    "\n",
    "where $f_{\\theta}$ is a neural network with trainable weights $\\theta$ that takes $t$ and $y$ as inputs and outputs an approximation for $dy/dt$.\n",
    "\n",
    "The neural network weights are trained by minimizing a loss function, such as the mean squared error (MSE), between the true solutions $y_k$, observed at discrete time points $t_k$, and the predicted $\\hat{y}_k$ obtained by numerically solving the neural ODE. The typical workflow is as follows:\n",
    "\n",
    "1. Numerically integrate the neural ODE from a given initial condition $y_0$,\n",
    "\n",
    "$$\\hat{y}(t)=y_0 + \\int_0^{t}f_\\theta(\\tau,y)d\\tau.$$\n",
    "\n",
    "2. Compute the loss function between the observed and predicted solutions at the observed time points $t_k$,\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum_{k=1}^N(y_k - \\hat{y}_k)^2.$$\n",
    "\n",
    "3. Backpropagate\\* and update the neural network weights using a chosen optimizer.\n",
    "\n",
    "\\*Note that backpropagation through the integral can be memory intensive and/or numerically unstable for some solvers.  For this reason, the adjoint method (implemented in `torchdiffeq`) can be used to compute the necessary gradients (more details in the original [paper](https://arxiv.org/pdf/1806.07366.pdf)). For more in-depth study, I highly recommend these [course notes](https://book.sciml.ai/), which also include a detailed derivation of the [adjoint for an ODE](https://book.sciml.ai/notes/11-Differentiable_Programming_and_Neural_Differential_Equations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf7b0d",
   "metadata": {},
   "source": [
    "### ODE Basics\n",
    "\n",
    "Start by writing a general `ODE` class which can be used to define both true and neural differential equations. The method `solve` uses `odeint` or `odeint_adjoint` to numerically integrate the ODE at times `t` starting from an initial condition `y0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE(nn.Module):  \n",
    "    def __init__(self, method='dopri5', adjoint=False, requires_grad=True, default_type=torch.float64):      \n",
    "        super(ODE, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.adjoint = adjoint if requires_grad else False\n",
    "        self.odeint = odeint_adjoint if self.adjoint else odeint\n",
    "        self.default_type = default_type\n",
    "        \n",
    "    \n",
    "    def solve(self, t, y0, device='cpu', rtol=1e-7, atol=1e-9):\n",
    "        if self.training:\n",
    "            return self.odeint(self.to(device), y0.to(device), t.to(device), method=self.method,\n",
    "                               rtol=rtol, atol=atol, options={'dtype':self.default_type})\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return odeint(self.to(device), y0.to(device), t.to(device), method=self.method,\n",
    "                              rtol=rtol, atol=atol, options={'dtype':self.default_type})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dfca2d",
   "metadata": {},
   "source": [
    "### Example ODE\n",
    "\n",
    "For this tutorial, we're going to train a neural network to model the [Fisher-Kolmogorov–Petrovsky–Piskunov](https://en.wikipedia.org/wiki/Fisher%27s_equation) (Fisher-KPP) PDE of a reaction diffusion system,\n",
    "\n",
    "$$\\frac{d\\rho}{dt} = r\\rho(1-\\rho) + D\\frac{d^2\\rho}{dx^2},$$\n",
    "\n",
    "where $r$ and $D$ are the reaction and diffusion constants, respectively.\n",
    "\n",
    "First, we will generate training data by numerically solving the true Fisher-KPP equation with periodic boundary conditions from a set of initial conditions of the form,\n",
    "\n",
    "$$\\rho_0(x) = a\\exp{\\left(-\\frac{x - b}{2c^2}\\right)}.$$\n",
    "\n",
    "We can discretize the PDE over a spatial grid using finite differences, converting it to a system of ODEs:\n",
    "\n",
    "$$\\frac{d\\rho_j}{dt} = r\\rho_j(1-\\rho_j) + D\\frac{\\rho_{j+1} - 2\\rho_j + \\rho_{j-1}}{h^2},$$\n",
    "\n",
    "where $h = L/N$ denotes the spacing for a system of length $L$ discretized into a grid of size $N$. This can then be numerically integrated in time using an ODE solver. Here we've specified the Dormand-Prince method (`dopri5`) as implemented in `torchdiffeq` as the default solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FisherKPP(ODE):\n",
    "    def __init__(self, args, method='dopri5', default_type=torch.float64):\n",
    "        super(FisherKPP, self).__init__(method, adjoint=False, requires_grad=False, default_type=default_type)\n",
    "        \n",
    "        default_args = {'N': 20,\n",
    "                        'L': 1,\n",
    "                        'r': 0.5,\n",
    "                        'D': 1e-3\n",
    "                       }\n",
    "        for k, v in default_args.items():\n",
    "            setattr(self, k, args[k] if k in args else v)\n",
    "        \n",
    "        # Reaction term\n",
    "        self.f = lambda y: self.r*y*(1 - y)\n",
    "        \n",
    "        # Diffusion term\n",
    "        self.h = self.L/self.N\n",
    "        kernel = (1./self.h)**2*torch.tensor([[[1,-2,1]]], dtype=default_type)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel.shape[-1], bias=False, padding='same', padding_mode='circular')\n",
    "        self.conv.weight = nn.Parameter(self.D*kernel, requires_grad=False)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "            \n",
    "    def init_state(self, M=1, args={}, seed=4):\n",
    "        # Generate initial condition(s)\n",
    "        torch.manual_seed(seed) \n",
    "        x = torch.arange(0, self.L, self.h)\n",
    "        \n",
    "        default_args = {'a': 0.2,\n",
    "                        'b': torch.rand(size=(M,1,1)),\n",
    "                        'c': 2*self.h\n",
    "                       }\n",
    "        for k, v in default_args.items():\n",
    "            if k not in args.keys():\n",
    "                args[k] = v\n",
    "        \n",
    "        return args['a']*(torch.exp(-0.5*(x - self.L*args['b'])**2/args['c']**2))\n",
    "        \n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        return self.f(y) + self.conv(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0ae0c",
   "metadata": {},
   "source": [
    "Numerically integrate the ODE for a specific choice of parameters $N,L,r$ and $D$ and evaluate at evenly-spaced time points $t_k$ between $0$ and $t_f$. In this example, we generate $M$ different initial conditions with $N$ evenly-spaced values of $x$ between $0$ and $L$, setting $a=0.2$ and $c=2h$ and randomly sampling $b$ between $0$ and $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'N': 20,\n",
    "        'L': 1,\n",
    "        'r': 0.5,\n",
    "        'D': 1e-3\n",
    "       }\n",
    "M = 100\n",
    "\n",
    "ode = FisherKPP(args, method='dopri5', default_type=default_type).to(device)\n",
    "y0 = ode.init_state(M)\n",
    "\n",
    "tf = 10\n",
    "dt = 0.1\n",
    "t = torch.arange(0, tf, dt)\n",
    "y = ode.solve(t, y0, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2123c1",
   "metadata": {},
   "source": [
    "Plot the true solution of the $i^{th}$ initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5561d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, ax = plt.subplots(figsize=(3.5,2.5))\n",
    "sm = ax.imshow(y[:,i].squeeze().cpu().T, aspect='auto', interpolation='nearest', extent=(0,tf+dt,0,ode.L),\n",
    "               cmap=cmap, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "format_axis(ax, props, 't', 'x', xbins=6, ybins=3)\n",
    "cbar = plt.colorbar(sm, ax=ax, aspect=16)\n",
    "format_axis(cbar.ax, props, ylabel=r'$\\rho(t,x)$', ybins=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1fd4a",
   "metadata": {},
   "source": [
    "### Neural ODE\n",
    "\n",
    "Suppose now that the exact form of the Fisher-KPP equation is unknown; instead, we will model it as a sum of neural networks that approximate the reaction and diffusion terms,\n",
    "\n",
    "$$\\frac{d\\rho}{dt} \\approx f_\\theta(\\rho) + g_\\phi(\\rho),$$\n",
    "\n",
    "where $f_\\theta(\\rho)$ is a simple feed-forward neural network modeling the reaction term, and $g_\\phi(\\rho)$ is a convolutional neural network modeling the diffusion term. $\\theta$ and $\\phi$ denote the trainable parameters of each model, respectively. In this example, $f_\\theta(\\rho)$ is specified by the number of neurons in the hidden layer, `hidden_dim`, while $g_\\phi(\\rho)$ is specified by the size of the convolutional kernel, `kernel_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6664268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(ODE):\n",
    "    def __init__(self, args, method='dopri5', adjoint=False, default_type=torch.float64):   \n",
    "        super(NeuralODE, self).__init__(method, adjoint, requires_grad=True, default_type=default_type)\n",
    "        \n",
    "        default_args = {'hidden_dim': 10,\n",
    "                        'kernel_size': 3,\n",
    "                       }\n",
    "        for k, v in default_args.items():\n",
    "            setattr(self, k, args[k] if k in args else v)\n",
    "        \n",
    "        # Reaction term\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(1, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Diffusion term\n",
    "        self.conv = nn.Conv1d(1, 1, self.kernel_size, bias=False, padding='same', padding_mode='circular')\n",
    "        \n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        return self.f(y[...,None]).view(y.shape) + self.conv(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'hidden_dim': 10,\n",
    "        'kernel_size': 3\n",
    "       }\n",
    "\n",
    "node = NeuralODE(args, method='dopri5', adjoint=False, default_type=default_type).to(device)\n",
    "optimizer = optim.Adam(node.parameters(), lr=1e-3)\n",
    "model_path = 'fkpp_node.torch'\n",
    "print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e48d6",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train our model, we will also specify the following parameters:\n",
    "- `max_iters` - Maximum number of iterations, *i.e.* how many batches of training data the model will see\n",
    "- `batch_time` - Number of time steps to sample in one training batch\n",
    "- `batch_size` - Number of initial conditions to sample in one training batch\n",
    "- `chkpt` - Number of iterations after which to record the current output\n",
    "\n",
    "At each checkpoint, we will record the loss, make a prediction of the current solution from a single initial condition, and record the current equation learned by the neural ODE. This will allow us to keep track of how the model is training. In the end, we can visualize the training history, which should look something like this:\n",
    "![fkpp_node_history](fkpp_node_history.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "batch_time = 5\n",
    "batch_size = 10\n",
    "\n",
    "try: saved = torch.load(model_path)\n",
    "except:\n",
    "    chkpt = 100\n",
    "    loss = []\n",
    "    \n",
    "    t_eval = torch.arange(0, tf, dt)\n",
    "    y_true = ode.solve(t_eval, y0[[0]], device).squeeze().cpu()\n",
    "    y_pred = []\n",
    "    \n",
    "    dx = ode.L/ode.N\n",
    "    y_eval = torch.arange(0, ode.L + dx, dx)\n",
    "    f_true = ode(torch.tensor(0), y_eval[:,None,None].to(device)).squeeze().cpu()\n",
    "    f_pred = []\n",
    "else:\n",
    "    chkpt = saved['chkpt']\n",
    "    loss = saved['loss']\n",
    "    \n",
    "    t_eval = saved['t_eval']\n",
    "    y_true = saved['y_true']\n",
    "    y_pred = saved['y_pred']\n",
    "    \n",
    "    y_eval = saved['y_eval']\n",
    "    f_true = saved['f_true']\n",
    "    f_pred = saved['f_pred']\n",
    "    \n",
    "    node.load_state_dict(saved['state'])\n",
    "    node.to(device)\n",
    "    optimizer.load_state_dict(saved['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce40cec",
   "metadata": {},
   "source": [
    "The training loop below can be run as many times as desired until the loss is sufficiently low or makes no further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "for k in range(1, max_iters + 1):\n",
    "    node.train()\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    t_batch, y0_batch, y_batch = get_batch(t, y0, y, batch_time, batch_size)\n",
    "    y_pred_batch = node.solve(t_batch, y0_batch, device)\n",
    "    _loss = nn.MSELoss()(y_pred_batch, y_batch)\n",
    "    \n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if k % chkpt == 0:\n",
    "        with torch.no_grad():\n",
    "            loss.append(_loss.item())\n",
    "            print('Iteration: {:04d} | Total Loss: {:.2e} | Time: {:.6f}'.format(k, _loss.item(), time.time() - end))\n",
    "        \n",
    "            node.eval()\n",
    "            y_pred.append(node.solve(t_eval, y0[[0]], device).squeeze().cpu())\n",
    "            f_pred.append(node(torch.tensor(0), y_eval[:,None,None].to(device)).squeeze().cpu())\n",
    "        \n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8301c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,2.5))\n",
    "ax.plot([chkpt*k for k in range(len(loss))], loss)\n",
    "ax.set_yscale('log')\n",
    "format_axis(ax, props, 'Iterations', 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'state': node.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'chkpt': chkpt,\n",
    "    'loss': loss,\n",
    "    't_eval': t_eval,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_eval': y_eval,\n",
    "    'f_true': f_true,\n",
    "    'f_pred': f_pred,\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an animation of the training history\n",
    "ani = animate_node_history(model_path)\n",
    "ani.save('fkpp_node_history.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79447d18",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now that we have a trained neural ODE, we can evaluate it on different initial conditions or go beyond the time points seen during training to see how it extrapolates beyond the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00411ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "y0_inf = ode.init_state(m, {'a': 0.1*torch.rand(m,1,1), 'b': torch.rand(m,1,1), 'c': 0.1*torch.rand(m,1,1)})\n",
    "y0_inf = y0_inf.mean(dim=0, keepdim=True)\n",
    "t_inf = torch.arange(0, 2*tf, dt)\n",
    "\n",
    "fig = node_inference(ode, node, t_inf, y0_inf, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbb0b9",
   "metadata": {},
   "source": [
    "## Part II: Physics-Informed Neural Networks\n",
    "\n",
    "Physics-informed neural networks are neural network models which approximate the solution of a known ODE or PDE that is typically difficult or computationally-expensive to integrate numerically. For example, let's consider the Fisher-KPP PDE of the previous section,\n",
    "\n",
    "$$\\frac{d\\rho}{dt} = r\\rho(1-\\rho) + D\\frac{d^2\\rho}{dx^2}.$$\n",
    "\n",
    "We can hypothesize that the solution $\\rho(t,x)$ to a specific initial/boundary value problem can be modeled by a neural network $f_\\theta$, parameterized by trainable weights $\\theta$,\n",
    "\n",
    "$$\\rho(t,x) \\approx f_\\theta(t,x).$$\n",
    "\n",
    "If this were true, then the neural network would be expected to satisfy the PDE of the system; that is,\n",
    "\n",
    "$$\\frac{df_\\theta}{dt} = rf_\\theta(1-f_\\theta) + D\\frac{d^2f_\\theta}{dx^2}.$$\n",
    "\n",
    "The neural network weights are thereby trainable by minimizing a loss function, such as the MSE, between the left- and right-hand sides of the equation above for a set of randomly sampled $\\{t_k, x_k\\}$ in the domain,\n",
    "\n",
    "$$\\mathcal{L}_{PDE} = \\frac{1}{N}\\sum_{k=1}^N\\left[\\frac{df_\\theta}{dt}\\bigg|_{t_k,x_k} - \\left(rf_\\theta(1-f_\\theta) + D\\frac{d^2f_\\theta}{dx^2}\\right)\\bigg|_{t_k,x_k}\\right]^2.$$\n",
    "\n",
    "Additionally, since the solution corresponds to a specific choice of initial and boundary conditions, these should also be used to constrain the neural network. For instance, for the initial condition $\\rho_0(x)$ and periodic boundary conditions,\n",
    "\n",
    "$$\\mathcal{L}_{IC} = \\frac{1}{N_I}\\sum_{k=1}^{N_I}\\left(\\rho_0(x_k) - f_\\theta(0,x_k)\\right)^2,$$\n",
    "\n",
    "where $N_I$ is the number of points $x_k$ sampled at the initial time, and\n",
    "\n",
    "$$\\mathcal{L}_{BC} = \\frac{1}{N_B}\\sum_{k=1}^{N_B}\\left(f_\\theta(t_k,0) - f_\\theta(t_k,L)\\right)^2,$$\n",
    "\n",
    "where $N_B$ is the number of points $t_k$ sampled at the boundaries. The total loss used to optimize the neural network is then given by,\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{PDE} + \\mathcal{L}_{IC} + \\mathcal{L}_{BC}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1f8cc",
   "metadata": {},
   "source": [
    "### Example PINN\n",
    "\n",
    "Let's implement a simple feed-forward neural network to model the solution to the Fisher-KPP equation with periodic boundary conditions under a specific choice of initial condition.\n",
    "\n",
    "The neural network is specified by the number of neurons in the hidden layers, `hidden_dim`, and the number of layers, `num_layers`.\n",
    "\n",
    "The initial condition is specified by the parameters $a,b$ and $c$. Here, we select $a=0.2, b=0.5$, and $c=0.1$.\n",
    "\n",
    "Finally, we use automatic differentiation to compute the derivatives of $f_\\theta$ with respect to the inputs $t$ and $x$. The method `pde` below outputs the calculated left- and right-hand sides of the PDE using the current state of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d186249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, args, default_type=torch.float64):   \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        default_args = {'r': 0.5,\n",
    "                        'D': 1e-3,\n",
    "                        'hidden_dim': 16,\n",
    "                        'num_layers': 8\n",
    "                       }\n",
    "        for k, v in default_args.items():\n",
    "            setattr(self, k, args[k] if k in args else v)\n",
    "        \n",
    "        modules = [nn.Sequential(\n",
    "                   nn.Linear(2, self.hidden_dim),\n",
    "                   nn.Tanh(),\n",
    "            )] \n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            modules.append(nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.Tanh(),\n",
    "            ))     \n",
    "        modules.append(nn.Linear(self.hidden_dim, 1))\n",
    "        \n",
    "        self.f = nn.Sequential(*modules)\n",
    "    \n",
    "    \n",
    "    def init_state(self, x, args={}):\n",
    "        # Generate initial condition\n",
    "        default_args = {'a': 0.2,\n",
    "                        'b': 0.5,\n",
    "                        'c': 0.1\n",
    "                       }\n",
    "        for k, v in default_args.items():\n",
    "            if k not in args.keys():\n",
    "                args[k] = v\n",
    "                \n",
    "        return args['a']*(torch.exp(-0.5*(x - args['b'])**2/args['c']**2))\n",
    "    \n",
    "        \n",
    "    def pde(self, t, x):\n",
    "        # Calculate PDE\n",
    "        y = self(t, x)\n",
    "        y_x = torch.autograd.grad(y, x, create_graph=True, grad_outputs=torch.ones_like(y))[0]\n",
    "        y_xx = torch.autograd.grad(y_x, x, create_graph=True, grad_outputs=torch.ones_like(y))[0]\n",
    "        y_t = torch.autograd.grad(y, t, create_graph=True, grad_outputs=torch.ones_like(y))[0]\n",
    "        return y_t, self.r*y*(1 - y) + self.D*y_xx\n",
    "\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        z = torch.cat([t, x], dim=-1)\n",
    "        return self.f(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed00def",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'r': 0.5,\n",
    "        'D': 1e-3,\n",
    "        'hidden_dim': 32,\n",
    "        'num_layers': 8\n",
    "       }\n",
    "\n",
    "pinn = NeuralNetwork(args, default_type).to(device)\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=3e-4)\n",
    "model_path = 'fkpp_pinn.torch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc7313",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train our model, we will also specify the following parameters:\n",
    "- `max_iters` - Maximum number of iterations, *i.e.* how many batches of training data the model will see\n",
    "- `batch_size` - Number of interior points to sample in one training batch\n",
    "- `chkpt` - Number of iterations after which to record the current output\n",
    "\n",
    "At each checkpoint, we will record the loss, make a prediction of the current solution, and record the current discrepancy in the PDE. This will allow us to keep track of how the model is training. In the end, we can visualize the training history, which should look something like this:\n",
    "![fkpp_pinn_history](fkpp_pinn_history.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "L = 1.\n",
    "tf = 10\n",
    "dt = 0.1\n",
    "t = torch.arange(0, tf, dt)[:,None].to(device)\n",
    "x = torch.arange(0, L, L/N)[:,None].to(device)\n",
    "\n",
    "# Initial state\n",
    "t0 = torch.zeros_like(x)\n",
    "y0 = pinn.init_state(x).to(device)\n",
    "\n",
    "# Boundaries\n",
    "xr = torch.zeros_like(t)\n",
    "xl = L*torch.ones_like(t)\n",
    "\n",
    "max_iters = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "try: saved = torch.load(model_path)\n",
    "except:\n",
    "    chkpt = 100\n",
    "    loss = []\n",
    "    \n",
    "    t_eval, x_eval = torch.meshgrid((t.squeeze(), x.squeeze()), indexing='xy')\n",
    "    t_eval, x_eval = t_eval.reshape(-1,1), x_eval.reshape(-1,1)\n",
    "    t_eval.requires_grad = True\n",
    "    x_eval.requires_grad = True\n",
    "    y_true = ode.solve(t.squeeze(), y0.T[None,:], device).squeeze().cpu().T\n",
    "    y_pred = []\n",
    "    f_pred = []\n",
    "else:\n",
    "    chkpt = saved['chkpt']\n",
    "    loss = saved['loss']\n",
    "    \n",
    "    t_eval = saved['t_eval'].to(device)\n",
    "    x_eval = saved['x_eval'].to(device)\n",
    "    y_true = saved['y_true']\n",
    "    y_pred = saved['y_pred']\n",
    "    f_pred = saved['f_pred']\n",
    "    \n",
    "    t_eval.requires_grad = True\n",
    "    x_eval.requires_grad = True\n",
    "    pinn.load_state_dict(saved['state'])\n",
    "    pinn.to(device)\n",
    "    optimizer.load_state_dict(saved['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1354649",
   "metadata": {},
   "source": [
    "The training loop below can be run as many times as desired until the loss is sufficiently low or makes no further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "for k in range(1, max_iters + 1):\n",
    "    pinn.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Initial condition loss\n",
    "    y0_pred = pinn(t0, x)\n",
    "    loss_ic = nn.MSELoss()(y0_pred, y0)\n",
    "    \n",
    "    # Boundary condition loss\n",
    "    yr_pred = pinn(t, xr)\n",
    "    yl_pred = pinn(t, xl)\n",
    "    loss_bc = nn.MSELoss()(yr_pred, yl_pred)\n",
    "    \n",
    "    # PDE loss\n",
    "    t_batch = tf*torch.rand((batch_size,1), requires_grad=True, device=device)\n",
    "    x_batch = L*torch.rand((batch_size,1), requires_grad=True, device=device)\n",
    "    loss_pde = nn.MSELoss()(*pinn.pde(t_batch, x_batch))\n",
    "    \n",
    "    # Total loss\n",
    "    _loss = loss_ic + loss_bc + loss_pde\n",
    "    \n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if k % chkpt == 0:\n",
    "        with torch.no_grad():\n",
    "            loss.append(_loss.item())\n",
    "            print('Iteration: {:04d} | Total Loss: {:.2e} | Time: {:.6f}'.format(k, _loss.item(), time.time() - end))\n",
    "        \n",
    "            pinn.eval()\n",
    "            y_pred.append(pinn(t_eval, x_eval).reshape(len(x),len(t)).squeeze().cpu())\n",
    "        \n",
    "        y_lhs, y_rhs = pinn.pde(t_eval, x_eval)\n",
    "        y_lhs = y_lhs.reshape(len(x),len(t)).squeeze().detach().cpu()\n",
    "        y_rhs = y_rhs.reshape(len(x),len(t)).squeeze().detach().cpu()\n",
    "        f_pred.append(y_lhs - y_rhs)\n",
    "            \n",
    "    end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d216b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,2.5))\n",
    "ax.plot([chkpt*k for k in range(len(loss))], loss)\n",
    "ax.set_yscale('log')\n",
    "format_axis(ax, props, 'Iterations', 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'state': pinn.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'chkpt': chkpt,\n",
    "    'loss': loss,\n",
    "    't_eval': t_eval.detach().cpu(),\n",
    "    'x_eval': x_eval.detach().cpu(),\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'f_pred': f_pred,\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb353fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an animation of the training history\n",
    "ani = animate_pinn_history(model_path)\n",
    "ani.save('fkpp_pinn_history.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e86875",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Now that we have a trained PINN, we can evaluate it at any $x$ in the domain and even beyond time points seen during training to see how it extrapolates beyond the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "L = 1.\n",
    "args = {'N': N,\n",
    "        'L': L,\n",
    "        'r': 0.5,\n",
    "        'D': 1e-3\n",
    "       }\n",
    "ode = FisherKPP(args, method='dopri5', default_type=default_type).to(device)\n",
    "\n",
    "t_inf = torch.arange(0, 2*tf, dt)\n",
    "x_inf = torch.arange(0, L, L/N)\n",
    "    \n",
    "fig = pinn_inference(ode, pinn, t_inf, x_inf, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b466eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
